{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import LeaveOneOut,StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest, VarianceThreshold, RFECV, SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.learning_curve import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_generation(file_name_1,file_name_2,output_file_dir):\n",
    "    x = [] \n",
    "    y = []\n",
    "    with open(file_name_1) as f:\n",
    "        feature_name = f.readline()\n",
    "        feature_name = feature_name.rstrip(os.linesep).split(',')\n",
    "        feature_name = feature_name[1:]\n",
    "        for line in f:\n",
    "            x.append([])\n",
    "            temp = line.rstrip(os.linesep).split(',')\n",
    "            temp = temp[1:]\n",
    "            for i in temp:\n",
    "                if i == '':\n",
    "                    x[-1].append(np.nan)\n",
    "                else:\n",
    "                    x[-1].append(float(i))\n",
    "        f.close()\n",
    "    with open(file_name_2) as f:\n",
    "        for line in f:\n",
    "            temp = line.rstrip(os.linesep)\n",
    "            y.append(float(temp))\n",
    "        f.close()\n",
    "    x = np.array(x)\n",
    "    x = x.astype('float32')\n",
    "    y2 = []\n",
    "    for i in y:\n",
    "        if i >= 0.3:\n",
    "            y2.append(0)\n",
    "        elif i>= -1:\n",
    "            y2.append(1)\n",
    "        else:\n",
    "            y2.append(2)\n",
    "    isnan = ~np.isnan(x).any(axis=0)\n",
    "    x = x[:,isnan]\n",
    "    feature_name = [feature_name[i] for i in range(len(isnan)) if isnan[i] ]\n",
    "    isinf = ~np.isinf(x).any(axis=0)\n",
    "    x = x[:,isinf]\n",
    "    feature_name = [feature_name[i] for i in range(len(isinf)) if isinf[i] ]\n",
    "    sel = VarianceThreshold()\n",
    "    x = sel.fit_transform(x)\n",
    "    feature_name = [feature_name[i] for i in range(len(sel.get_support())) if sel.get_support()[i]]\n",
    "    isneginf = ~np.isneginf(x).any(axis=0)\n",
    "    x = x[:,isneginf]\n",
    "    feature_name = [feature_name[i] for i in range(len(isneginf)) if isneginf[i] ]\n",
    "    clf = ExtraTreesClassifier(100000)\n",
    "    clf = clf.fit(x, y2)\n",
    "    importances = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_],axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    for feature in range(x.shape[1]):\n",
    "        print(\"%d. %s (%f)\" % (feature + 1, feature_name[indices[feature]], importances[indices[feature]]))\n",
    "    plt.figure()\n",
    "    plt.plot(importances[indices])\n",
    "    plt.title('Feature Importance in Descending Order')\n",
    "    plt.xlabel('Ranking')\n",
    "    plt.ylabel('Importance score')\n",
    "    plt.savefig(output_file_dir+'feature_importance_des_order.png')\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    sns.distplot(importances[indices])\n",
    "    plt.title('Feature Importance Distribution')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Importance score')\n",
    "    plt.savefig(output_file_dir+'feature_importance_distribution.png')\n",
    "    plt.show()\n",
    "    threshold = np.mean(importances[indices])+2*np.std(importances[indices])\n",
    "    for i in range(len(importances)):\n",
    "        if importances[indices[i]]<threshold:\n",
    "            cutoff_feature_index = i\n",
    "            break\n",
    "    print('Number of features left:'+str(cutoff_feature_index))\n",
    "    x_selected = x[:,indices[0:cutoff_feature_index]]\n",
    "    feature_name = feature_name[indices[0:cutoff_feature_index]]\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lr = LogisticRegression()\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "\n",
    "    print('LDA:')\n",
    "    lda.fit(x_selected,y2)\n",
    "    y_pred = lda.predict(x_selected)\n",
    "    print(classification_report(y2, y_pred))\n",
    "\n",
    "    print('Logistic regression:')\n",
    "    lr.fit(x_selected,y2)\n",
    "    y_pred = lr.predict(x_selected)\n",
    "    print(classification_report(y2, y_pred))\n",
    "\n",
    "    print('Support vector classification with linear kernel:')\n",
    "    svc.fit(x_selected,y2)\n",
    "    y_pred = svc.predict(x_selected)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    \n",
    "    df = pd.DataFrame(x_selected,columns=[feature_name[indices[i]] for i in range(cutoff_feature_index)])\n",
    "    corr = df.corr()\n",
    "    microarray_cmap = LinearSegmentedColormap('microarray', {\n",
    "            'red': [(0.0, 1.0, 1.0), (0.5, 0.2, 0.2), (1.0, 0.0, 0.0)],\n",
    "            'green': [(0.0, 0.0, 0.0), (0.5, 0.2, 0.2), (1.0, 1.0, 1.0)],\n",
    "            'blue': [(0.0, 0.0, 0.0), (0.5, 0.2, 0.2), (1.0, 0.0, 0.0)],\n",
    "    })\n",
    "    microarray_cmap.set_bad('w')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    mask =  np.tri(corr.shape[0],k=-1).T\n",
    "    cax = ax.matshow(np.ma.array(corr,mask=mask), interpolation='none', cmap=microarray_cmap)\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels(['']+list(corr.columns),rotation='vertical')\n",
    "    ax.set_yticklabels(['']+list(corr.columns))\n",
    "    tick_spacing = 1\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    ax.yaxis.set_label_coords(0, 0)\n",
    "    ax.grid(False)\n",
    "    plt.savefig(output_file_dir+'correlation.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Recursive feature elimination:')\n",
    "    \n",
    "    print('LDA + recall_macro:')\n",
    "    rfecv = RFECV(estimator=lda, step=1, cv=StratifiedKFold(y2, 20), scoring='recall_macro')\n",
    "    rfecv.fit(x_selected, y2)\n",
    "    model = rfecv.estimator_\n",
    "    x_selected_selected = x_selected[:,rfecv.support_]\n",
    "    feature_name_selected = feature_name[rfecv.support_]\n",
    "    y_pred = model.predict(x_selected_selected)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(feature_name_selected)\n",
    "    print(model.coef_)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    print('Leave-one-out cross validation accuracy:')\n",
    "    print(str(np.mean(cross_val_score(lda,x_selected_selected,y2,cv=LeaveOneOut(len(y2)),scoring='accuracy'))))\n",
    "\n",
    "\n",
    "    print('LDA + recall_weighted:')\n",
    "    rfecv = RFECV(estimator=lda, step=1, cv=StratifiedKFold(y2, 20), scoring='recall_weighted')\n",
    "    rfecv.fit(x_selected, y2)\n",
    "    model = rfecv.estimator_\n",
    "    x_selected_selected = x_selected[:,rfecv.support_]\n",
    "    feature_name_selected = feature_name[rfecv.support_]\n",
    "    y_pred = model.predict(x_selected_selected)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(feature_name_selected)\n",
    "    print(model.coef_)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    print('Leave-one-out cross validation accuracy:\\n')\n",
    "    print(str(np.mean(cross_val_score(lda,x_selected_selected,y2,cv=LeaveOneOut(len(y2)),scoring='accuracy'))))\n",
    "\n",
    "    print('Logistic regression + recall_macro:')\n",
    "    rfecv = RFECV(estimator=lr, step=1, cv=StratifiedKFold(y2, 20), scoring='recall_macro')\n",
    "    rfecv.fit(x_selected, y2)\n",
    "    model = rfecv.estimator_\n",
    "    x_selected_selected = x_selected[:,rfecv.support_]\n",
    "    feature_name_selected = feature_name[rfecv.support_]\n",
    "    y_pred = model.predict(x_selected_selected)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(feature_name_selected)\n",
    "    print(model.coef_)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    lr.fit(x_selected_selected,y2)\n",
    "    y_pred = lr.predict(x_selected_selected)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(lr,x_selected_selected,y2,scoring='accuracy', cv=LeaveOneOut(len(x_selected_selected)))\n",
    "    print('Leave-one-out cross validation accuracy:')\n",
    "    print(str(np.mean(cross_val_score(lr,x_selected_selected,y2,cv=LeaveOneOut(len(y2)),scoring='accuracy'))))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Sample Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_file_dir+'learning_curve_logistic_recall_macro.png')\n",
    "    plt.show()\n",
    "\n",
    "    print('Logistic regression + recall_weighted:')\n",
    "    rfecv = RFECV(estimator=lr, step=1, cv=StratifiedKFold(y2, 20), scoring='recall_weighted')\n",
    "    rfecv.fit(x_selected, y2)\n",
    "    model = rfecv.estimator_\n",
    "    x_selected_selected = x_selected[:,rfecv.support_]\n",
    "    feature_name_selected = feature_name[rfecv.support_]\n",
    "    y_pred = model.predict(x_selected_selected)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(feature_name_selected)\n",
    "    print(model.coef_)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    lr.fit(x_selected_selected,y2)\n",
    "    y_pred = lr.predict(x_selected_selected)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(lr,x_selected_selected,y2,scoring='accuracy', cv=LeaveOneOut(len(x_selected_selected)))\n",
    "    print('Leave-one-out cross validation accuracy:')\n",
    "    print(str(np.mean(cross_val_score(lr,x_selected_selected,y2,cv=LeaveOneOut(len(y2)),scoring='accuracy'))))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Sample Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_file_dir+'learning_curve_logistic_recall_weighted.png')\n",
    "    plt.show()\n",
    "\n",
    "    print('SVC(linear) + recall_macro:')\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y2, 20), scoring='recall_macro')\n",
    "    rfecv.fit(x_selected, y2)\n",
    "    model = rfecv.estimator_\n",
    "    x_selected_selected = x_selected[:,rfecv.support_]\n",
    "    feature_name_selected = feature_name[rfecv.support_]\n",
    "    y_pred = model.predict(x_selected_selected)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(feature_name_selected)\n",
    "    print(model.coef_)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    svc.fit(x_selected_selected,y2)\n",
    "    y_pred = svc.predict(x_selected_selected)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(svc,x_selected_selected,y2,scoring='accuracy', cv=LeaveOneOut(len(x_selected_selected)))\n",
    "    print('Leave-one-out cross validation accuracy:\\n')\n",
    "    print(str(np.mean(cross_val_score(svc,x_selected_selected,y2,cv=LeaveOneOut(len(y2)),scoring='accuracy'))))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Sample Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_file_dir+'learning_curve_svc_recall_macro.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print('SVC(linear) + recall_weighted:\\n')\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y2, 20), scoring='recall_weighted')\n",
    "    rfecv.fit(x_selected, y2)\n",
    "    model = rfecv.estimator_\n",
    "    x_selected_selected = x_selected[:,rfecv.support_]\n",
    "    feature_name_selected = feature_name[rfecv.support_]\n",
    "    y_pred = model.predict(x_selected_selected)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(feature_name_selected)\n",
    "    print(model.coef_)\n",
    "    print(classification_report(y2, y_pred))\n",
    "    svc.fit(x_selected_selected,y2)\n",
    "    y_pred = svc.predict(x_selected_selected)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(svc,x_selected_selected,y2,scoring='accuracy', cv=LeaveOneOut(len(x_selected_selected)))\n",
    "    print('Leave-one-out cross validation accuracy:\\n')\n",
    "    print(str(np.mean(cross_val_score(svc,x_selected_selected,y2,cv=LeaveOneOut(len(y2)),scoring='accuracy'))))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Sample Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_file_dir+'learning_curve_svc_recall_weighted.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PaDEL 1D+2D descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_generation('../data/features.csv','../data/logBB.txt','../data/features/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PubChem fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_generation('../data/features2.csv','../data/logBB.txt','../data/features2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PaDEL 1D+2D descriptors + PubChem fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_generation('../data/features3.csv','../data/logBB.txt','../data/features3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
